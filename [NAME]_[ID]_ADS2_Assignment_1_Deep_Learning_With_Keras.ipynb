{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1 - Deep Learning With Keras - 40%\n",
        "\n",
        "**IMPORTANT NOTE**: By default, this notebook is set to a CPU runtime, to help prevent you getting locked out of Google Colab. When training your models, you will need to switch to a GPU runtime, otherwise the training will take a very long time.\n",
        "\n",
        "**Deadline**: 21 Mar 2023, 23:59\n",
        "\n",
        "**Submission Requirements**: You should submit your Colab Notebook, with all the outputs printed, and a sharing link to the notebook in your Drive. As detailed above, you should submit a 2-page report in PDF or DOCX format.\n",
        "\n",
        "**Learning Outcomes**\n",
        "\n",
        "This Assignment assesses the following module Learning Outcomes (from Definitive Module Document):\n",
        "\n",
        "* have knowledge of and understand how GPUs can accelerate data processing\n",
        "* be able to write data processing pipelines that exploit Tensorflow\n",
        "* have knowledge of and understand how to develop GPU-accelerated data processing pipelines using the Tensorflow and RAPIDS frameworks\n",
        "\n",
        "**Assignment Details**\n",
        "\n",
        "This assignment will test your ability to implement and test a deep neural network using keras. By varying the properties of the model and the data input pipeline, you will explore and analyse the training time and performance of the model. There will be four tasks for you to complete, each of which requires you to complete a range of tests on the model, visualise the results, and comment on them in a short report. Your report should focus on explaining and critically analysing the results—you will be assessed not just on your ability to show what is happening, but explain WHY it is happening.\n",
        "\n",
        "All coding work for this assignment should be done inside a copy of the Colab Notebook provided on this page. Any submissions not in this format will not be marked.\n",
        "\n",
        "**Task 1**: A model description is provided in the Colab Notebook for this assignment. Implement this model, ensuring that you have the correct output shapes for each of the layers and the correct number of model parameters. Train the model on the dataset provided in the notebook—initial training settings are provided also. Create plots of the losses and metrics of the training and validation data, and a plot that shows example images from each class that have been correctly AND incorrectly labelled by the model. Analyse these results in your report.\n",
        "\n",
        "**Task 2**: Select two additional optimizers. Including the one provided in the initial training settings, test your model with each of these optimizers using a range of different learning rates. You may need to train the model for more epochs to ensure that it converges on a solution. Create plots that show the losses and metrics for each of these runs, and comment on the results in your report. Select the optimizer and learning rate that provided the best results, and move onto the next task.\n",
        "\n",
        "**Task 3**: The batch size can heavily influence the amount of time it takes to train a model. Vary the batch size used to train the model and, utilising the Early Stopping callback provided, create plots that show how the time per epoch and total training time changes. Comment on these results in your report.\n",
        "\n",
        "**Task 4**: The model as provided does not contain any regularisation techniques. Edit the model architecture to include at least two examples of regularisation. Retrain the model using the new architecture, and repeat the analysis performed in task 1. In your report, compare and contrast the results from this task, with those from the initial model configuration.\n",
        "\n"
      ],
      "metadata": {
        "id": "pAeyW4-cyEZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q49oFfxbx3g_"
      },
      "outputs": [],
      "source": [
        "# Module Imports - Add any additional modules here\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, optimizers, losses, callbacks,\\\n",
        "                             regularizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset. Here we use the CIFAR-10 dataset of labelled images\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Rescale the pixel values\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "\n",
        "# List of label names\n",
        "class_names = ['plane', 'car', 'bird', 'cat', 'deer',\n",
        "               'canine', 'frog', 'horse', 'boat', 'truck']"
      ],
      "metadata": {
        "id": "XDxDRN2LzEzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Initial Model\n",
        "\n",
        "Implement the model architecture detailed below, using the Keras Functional API, ensuring that you have the correct output shapes for each of the layers.\n",
        "\n",
        "Train the model on the CIFAR-10 dataset.\n",
        "\n",
        "Create plots of the losses and metrics of the training and validation data, and plots that show example test images from each class that have been correctly AND incorrectly labelled by the model.\n",
        "\n",
        "Analyse these results in your report.\n",
        "\n",
        "**Model Architecture**\n",
        "\n",
        "A summary of the model architecture is given here, which shows the layers of the model, the output shapes of those layers, and the activation functions used. You will need to work out the other settings used to produce the model, such as the kernal sizes, padding schemes, and stride lengths. You should ensure that the output shapes and total number of parameters in your model match the summary here.\n",
        "\n",
        "```\n",
        "Model: \"cifar_model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Activation   \n",
        "=================================================================\n",
        " Input (InputLayer)          [(None, 32, 32, 3)]       None         \n",
        "                                                                 \n",
        " conv_1 (Conv2D)             (None, 32, 32, 16)        ReLU       \n",
        "                                                                 \n",
        " conv_2 (Conv2D)             (None, 32, 32, 16)        ReLU      \n",
        "                                                                 \n",
        " pool_1 (MaxPooling2D)       (None, 16, 16, 16)        None         \n",
        "                                                                 \n",
        " conv_3 (Conv2D)             (None, 16, 16, 32)        ReLU      \n",
        "                                                                 \n",
        " conv_4 (Conv2D)             (None, 16, 16, 32)        ReLU      \n",
        "                                                                 \n",
        " pool_2 (MaxPooling2D)       (None, 8, 8, 32)          None         \n",
        "                                                                 \n",
        " conv_5 (Conv2D)             (None, 8, 8, 64)          ReLU     \n",
        "                                                                 \n",
        " conv_6 (Conv2D)             (None, 8, 8, 64)          ReLU     \n",
        "                                                                 \n",
        " pool_3 (MaxPooling2D)       (None, 4, 4, 64)          None         \n",
        "                                                                 \n",
        " flat (Flatten)              (None, 1024)              None         \n",
        "                                                                 \n",
        " fc_1 (Dense)                (None, 512)               ReLU    \n",
        "                                                                 \n",
        " Output (Dense)              (None, 10)                SoftMax      \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 602,010\n",
        "Trainable params: 602,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "--hu0b080wJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the model using the provided architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "b7EwKqO-1N6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compile the model using the SGC optimizer, with default learning rate,\n",
        "### Sparse Categorical Crossentropy, and accuracy metric.\n",
        "\n"
      ],
      "metadata": {
        "id": "5nmP478X3qZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Train the model for 50 epochs, with a batch size of 128. Include the test\n",
        "### data for model validation. Store the losses and metrics in a history object.\n",
        "\n"
      ],
      "metadata": {
        "id": "cmnrcqwz37TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create plots of the losses and metrics of the training and validation data,\n",
        "### and plots that shows example test images from each class that have been\n",
        "### correctly AND incorrectly labelled by the model.\n"
      ],
      "metadata": {
        "id": "oQXwrFaG-LPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Testing Optimizers\n",
        "\n",
        "Select two additional optimizers. Including the SGD algorithm already used, test all three of these optimizers with a range of different learning rates.\n",
        "\n",
        "You may need to train the model for more or less epochs to ensure that it converges on a solution.\n",
        "\n",
        "Create plots that show the losses and metrics for each of these runs, and comment on the results in your report.\n",
        "\n",
        "Select the optimizer and learning rate that provided the best results, and move onto the next task.\n",
        "\n",
        "**Note**: You should reset the weights of the model between each test. A function is provided to perform this task. Store the losses and metrics of each run under a different variable name, so that they can all be plotted together."
      ],
      "metadata": {
        "id": "84jsOuq3-P-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function that resets the weights of your model. Call this before\n",
        "# recompiling your model with updated settings, to ensure you train the model\n",
        "# from scratch.\n",
        "\n",
        "def reinitialize(model):\n",
        "    # Loop over the layers of the model\n",
        "    for l in model.layers:\n",
        "        # Check if the layer has initializers\n",
        "        if hasattr(l,\"kernel_initializer\"):\n",
        "            # Reset the kernel weights\n",
        "            l.kernel.assign(l.kernel_initializer(tf.shape(l.kernel)))\n",
        "        if hasattr(l,\"bias_initializer\"):\n",
        "            # Reset the bias\n",
        "            l.bias.assign(l.bias_initializer(tf.shape(l.bias)))\n",
        "\n",
        "# Function modified from here: https://stackoverflow.com/questions/63435679/reset-all-weights-of-keras-model"
      ],
      "metadata": {
        "id": "KMR0npBE_75_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Test the SGD Optimizer, plus two others of your choice, with a range of\n",
        "### learning rates.\n"
      ],
      "metadata": {
        "id": "MSTCNsrJ5niP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create plots that show the losses and metrics for each of these runs, and\n",
        "### comment on the results in your report.\n"
      ],
      "metadata": {
        "id": "jLUdOnVZAeG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Testing Batch Sizes\n",
        "\n",
        "The batch size can heavily influence the amount of time it takes to train a model. Vary the batch size used to train the model and, utilising the Early Stopping callback provided, create plots that show how the time per epoch and total training time changes.\n",
        "\n",
        "Comment on these results in your report—consider both how the batch size influences the number of epochs it takes to reach a solution, and how long each epoch takes to run. Why is this the case?"
      ],
      "metadata": {
        "id": "_mv7U8Y_AlCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Train the model with a range of different batch sizes, resetting the weights\n",
        "### each time. Use an Early Stopping callback to prevent the model training for\n",
        "### too long.\n",
        "\n",
        "Early_Stop = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n"
      ],
      "metadata": {
        "id": "HcCVuUfDAn3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 - Adding Regularisation\n",
        "\n",
        "The model as provided does not contain any regularisation techniques. Edit the model architecture to include at least two examples of regularisation. Retrain the model using the new architecture, and repeat the analysis performed in task 1.\n",
        "\n",
        "In your report, compare and contrast the results from this task, with those from the initial model configuration. Explain HOW and WHY the results are different, with consideration to the predicted classifications, losses and metrics."
      ],
      "metadata": {
        "id": "fqZH46DmBSXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Update the model architecture to include at least two types of regularization.\n",
        "### Train the model using the ideal settings found in previous tasks.\n"
      ],
      "metadata": {
        "id": "AiLBxBQ3BvqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Repeat your analysis from task 1, creating plots of the losses, metrics AND\n",
        "### predicted classifications of images in the test set.\n"
      ],
      "metadata": {
        "id": "70TKDDKcB681"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}